---
title: 分布式系统-Raft算法介绍
date: 2018-11-28 10:23:46
categories:
- Distributed System
tags:
- Raft
- Leader Election
- Log Replication
- Consensus
comments: true
---

## 主节点与副本节点

通过副本节点保证系统可用性是分布式系统的一种常见的形式
每一个节点都存储有序的LogEntry，LogEntry的内容为节点需要执行的命令
因此要保证各个节点的一致性，只需要保证各个节点的Log一致性

## Raft算法的设计

Paxos算法存在难以理解和难以实现的问题
因此Raft算法的设计原则是保证算法的可理解性，从而让算法更容易实现

* 问题分解

Raft算法将分布式一致性的问题拆分为`选主(异常情况)`、`日志复制与心跳(正常情况)`、`一致性保证`、`成员变更`四个问题

* 减少节点内部可能出现的状态

## Raft算法的细节

### 基本流程

Raft算法首选需要从集群(推荐节点总数为5个)中选主一个Leader节点，该节点对需要复制的Log具有完全的控制权，集群中的所有节点可以处于Leader、Follower和Candidate三种状态
系统正常运行时，Leader节点对来自Client的LogEntry在Follower节点上进行复制
当Leader节点异常时，Follower节点变为Candidate节点，选主算法会在Candidate节点中选择出一个新的Leader节点，然后继续进行上述流程
只要集群中半数以上的节点正常，整个集群便可以正常对Client提供服务

每一次的选主开始的时间周期称之为一个Term，该Term在下一次选主开始前结束，Raft保证每一次选主要么失败，要么只会选择出一个新Leader

Term的变化过程

![img](/images/Raft之Term周期.png)

每一个节点都存储了自己所在的Term，并利用Term值进行状态转换
节点的状态转换过程

![img](/images/Raft之节点状态转换.png)

Raft中集群节点间的通信方式为RPC，RPC的类型包括

* RequestVote(选主)
* AppendEntries(日志复制和心跳)
* ServerRetry(日志同步)
* InstallSnapshot(Snapshot同步)

### 选主

* 触发机制

Leader节点通过不带LogEntries信息的AppendEntries RPC向Follower发送心跳
节点启动时，默认为Follower状态，当节点超过一段时间没有收到Leader的心跳信息之后，发生`Heartbeat Timeout`，触发选主操作

* 执行流程

节点将自己变为Candidate状态、将自己的Term加1、向自己投票并同时并行的向其他节点发送RequestVote RPC
集群中的每一个节点按照`谁先来，投谁票`+`Log检查` + `最小超时时间检查`的原则进行投票，并且在每一个Term中只能投一票
节点的选主可能会有三个结果：`赢得选举，成为Leader节点`，`输掉选举，成为Follower节点`，`选主超时，进行下一轮的选主`

* 结果分析

赢得选举： 每一个节点只要获得集群内超过半数节点的投票即表明赢得了选举，赢得选举之后，该节点便可以向其他节点发送AppendEntries RPC心跳信息来表明自己为Leader

输掉选举： 在节点等待投票结果的过程中，有可能会收到了其他节点的AppendEntries RPC请求，此时当前节点会根据该请求中的Term信息来决定自己是输掉选举还是再次发起选举。如果其它节点的Term值大于等于当前节点的Term值，则表明当前节点输掉了选举，节点变为Follower状态；反之，当前节点继续保持Candidate状态，并对RPC请求返回reject

选主超时：如果同时有多个节点都变为了Candidate状态并发起选举投票，那么有可能会导致没有任何一个节点能够获得超过一半的投票数，此时将会发生`Election Timeout`，此时节点将会重新发起新一轮的选主操作

* 随机超时时间

当发生`选主超时`时，如果不进行干预操作，那么该情况可能会永远发生下去
Raft使用随机超时时间来尽量避免出现该情况，并在出现该情况时能够及时修复
首先是使用随机的`Heartbeat Timeout`，减少多个节点同时变为Candidate的情况
其次是使用随机的`Election Timeout`，减少多个节点同时再次进行选举的情况

### 日志复制与心跳

* Log的组成

Log由有序的LogEntry列表组成，LogEntry包含`操作命令`、`来源的Term`、`在LogEntry列表中的下标Index`组成

* Log复制过程

当Follower收到Client请求后，会将请求转发到Leader中
当Leader收到Client请求后，使用AppendEntries RPC向Follower发送请求来进行日志复制
该该请求被超过半数的节点正确执行后，Leader决定将LogEntry加入到集群中，LogEntry变为已提交状态，对于没有正确执行该LogEntry的节点，Leader会持续发起请求
在AppendEntries RPC中包含Leader节点上已提交的LogEntry的最大Index和对应的Term，如果Follower发现该Index和Term与自己内部的Logs不相同，则拒绝该请求(**AppendEntries RPC会进行Leader与Follower的一致性检查**)，否则Follower根据该值决定是否将自己内部的LogEntry变为已提交状态

* Leader与Follower的一致性检查

当Leader或Follower异常之后，可能会导致节点间出现Logs不一致的情况，如Follower中比新Leader多出一些未提交的LogEntries、Follower中缺少新Leader中的某些Entries
Raft通过让Leader节点`要求Follower节点复制自己内部Logs`的形式来解决上述不一致的情况

Leader的内部维护了对于每一个Follower的nextIndex值，表示下一次发起AppendEntries RPC请求时，要发送给Follower的LogEntry的下标
当新Leader在出现时，每一个Follower的nextIndex被设置为Leader内Log总数加1，当某个Follower与Leader不一致时，该Follower会拒绝Leader下一次的AppendEntries RPC，此时Leader将Follower对应的nextIndex减1。重复上述过程，直到AppendEntries RPC的一致性检查通过，然后Follower会将内部与Leader不一致的LogEntry删除，并添加来自Leader的LogEntry，最终Follower于Leader达到一致

### 一致性保证

* 保证已提交的LogEntry不会丢失

选主得到的新Leader需要包含之前所有Term中已提交的LogEntry，这样LogEntry只会从Leader流向Follower，并且Leader永远不会重写自己的LogEntry
节点在进行选主而发起的RequestVote RPC中，包含了自己内部Log的最大Term和最大Index的信息，当其他节点收到该请求时，将自己的Log与其进行比较，如果自己的Log更新一点，则拒绝为其投票
Log的比较规则为：不同Term，Term大的更新；同一Term，Index更大的更新

* 已经在多数节点上被复制的LogEntry有可能会丢失

当Leader进行LogEntry复制之后、尚未进行提交之前发生异常时，新Leader并不会尝试提交之前Term里面的LogEntry

### 成员变更

* 手动实现 -> 导致集群不可用

关闭集群服务
修改配置
启动集群

* 自动实现 -> 在变更过程中，需要防止出现两个Leader的情况

  * 单次变更

    通过Leader向Follower发送一条特殊的LogEntry，告知集群配置信息，当该LogEntry已提交之后，单次变更结束
    每次只可以添加或删除一个节点，避免出现两个Leader的情况
    对于Follower，当LogEntry尚未提交时，不接受新的单次变更的LogEntry

  * 多次变更

    引入`Joint Consensus`状态,称为C-old,new,即新旧Configuration共存状态

    当Leader收到变更的请求之后向Follower发送一条特殊的C-new,old LogEntry，告知集群配置信息
    Follower在收到LogEntry之后，不管该LogEntry是否已提交，直接使用新的配置，**但此时C-new是不可以进行投票的**
    如果此时Leader异常，根据新Leader是否收到了LogEntry，该Leader要么在C-old中，要么在C-old,new中

    当该LogEntry被提交之后，Leader将所有Log复制到C-old和C-new节点中，同时选主算法保证了此时选主得到的Leader一定是包含了C-new,old LogEntry的节点
    此时Leader向所有C-new节点发送一条特殊的C-new LogEntry，当该LogEntry被提交之后，整个集群完成变更，c-old节点便可以直接关闭

    在上述过程中，不会出现c-old或c-new单边进行投票或日志复制的情况，因此可以保证不会出现两个Leader

    虽然多次变更方法很有效，但是比较难以理解，因此etcd实际使用的是单次变更的方式

![img](/images/Raft之Joint Consensus.png)

* 多次变更的问题与优化
  
  * Learner

    成员变更时，新加入的节点在复制数据时比较耗时(Snapshot机制可以缓解)，如果此时需要进行选主或日志复制，该节点可能响应速度较慢导致整个集群的不可用
    新加入的节点设置为Learner状态，没有选主和日志复制的投票权，当该节点与Leader日志同步完成之后，变为Follower状态

  * Leader不在C-new中

    在上述多次变更的运行过程中，有可能集群的Leader节点并不在新的集群配置当中
    该Leader节点在C-new LogEntry被提交之后，会进入Follower状态

  * 从集群中删除的节点

    这些节点由于不会受到Leader的心跳，会触发选主的操作
    节点在收到RequestVote RPC请求时，如果收到请求时间与Leader发送心跳的时间没有小于Heartbeat Timeout的最小值，则拒绝投票

### 日志压缩

随着系统的运行，Log所占的空间会不断增长，导致系统的可用性下降，因此需要某种机制对Log的存储进行优化

* Snapshot

将内部的最新状态持久化到可靠存储上，然后删除到当前为止的Log

在Raft中，每一个节点单独对已提交的LogEntry进行Snapshot操作，同时存储相应的元数据，包括`last included index`和`last included term`
`last included index`表示Snapshot之后被删除的最大的Log下标，`last included term`为该LogEntry对应的Term

由于各个节点是独立完成Snapshot的，当某个Follower长期处于异常或者运行异常缓慢的时候，就会出现某条LogEntry Leader已经通过Snapshot将其删除但是还未在Follower对其进行复制的情况，此时需要通过InstallSnapshot RPC对Follower同步Snapshot

* Log Cleaning与Log-Structured Merge Trees(LSM)

实现机制比SnapShot复杂，所以Raft算法未考虑

### Client与Raft集群的交互

* Client总是与Leader进行交互，如果Client与Follower交互时，Follower将会拒绝并向其发送Leader地址信息

* Client向Leader发送的命令包含唯一的序号，Leader中包含每一个Client的最新序号值，如果用户发送的命令的序号值已存在，则Leader不进行处理

## Raft算法的总结

![img](/images/Raft之总结.png)