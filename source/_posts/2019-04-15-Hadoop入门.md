---
title: Hadoop入门
date: 2019-04-15 15:03:45
categories: 
- 系统架构
tags: 
- Hadoop
- HDFS
- YARN
- MapReduce
---

Hadoop具有高容错、高可靠性和高扩展性的特点，特别适合一次写入、多次读取的场景

## 整体结构

- HDFS: 分布式文件存储
- YARN: 分布式资源管理
- MapReduce: 分布式计算
- Others: 利用YARN的资源管理功能实现其它的数据处理

每一个组件内部都采用Master-Worker架构

![img](/images/Hadoop之整体结构.png)

## HDFS

Hadoop Distributed File System，分布式文件系统

### 基本组件

- Block
  - 读写操作的基本单位，类似于磁盘中的页。一般为64/128MB
  - 一般磁盘传输速率比寻道时间要快，使用较大的存储块可以减少寻道时间
  - 使用较大的存储块可以减少管理块的开销，因为每一个块都需要在NameNode上有对应的记录
  - 每一个大文件会被拆分为一个一个的Block，然后存储到不同的机器中。如果文件大小小于单个Block，那么实际占用空间为该文件大小
  - 每一个Block都会被复制到多台机器中，默认是复制3份
- NameNode
  - 存储文件的元数据信息metadata。运行时将元数据都保存在内存中，因此HDFS可存储的文件数受限于NameNode节点的内存大小
  - 一个Block在NameNode中对应一条记录(约150Byte)，如果有大量的小文件时，会消耗大量内存；MapReduce任务在处理大量小文件时会产生大量任务，导致线程管理开销过大。因此Hadoop建议存储大文件
  - NameNode中不对Block的元数据信息进行持久化，而是由DataNode在注册时进行上报和运行时维护；NameNode在重启后重建DataNode中的Block信息
  - NameNode失效则整体HDFS失效，所以要保证NameNode的高可用
- Secondary NameNode
  - 定时与NameNode进行数据同步，当NameNode失效时需要手动将其NameNode地址设置为Secondary NameNode
- DataNode
  - 保存具体的Block数据
  - 负责数据的读写、复制等操作
  - 在启动时向NameNode报告当前存储的Block信息，Block变更后定时报告变更信息
  - 在DataNode之间进行Block的复制，保证数据的冗余和安全

![img](/images/Hadoop之HDFS结构.png)

### 写入操作

- Step1: 客户端将文件写入本地磁盘的临时文件中
- Step2: 当临时文件大小达到Block大小时，HDFS Client通知NameNode，申请写入文件
- Step3: NameNode在HDFS的文件系统中创建Block元数据，并将BlockID和要写入的DataNode列表返回给HDFS Client
- Step4/5: HDFS收到该信息后，将临时文件写入DataNodes
  - HDFS Client将文件写入到第一个DataNode中(一般以4KB为单位进行传输)
  - 第一个DataNode接收完成后，将数据写入本地磁盘同时也传输给第二个DataNode
  - 以此类推，以Pipeline的方式在DataNode间进行数据复制
  - 每一个DataNode在接收完成后，给前一个DataNode发送确认信号；最终由第一个DataNode向HDFS Client发送确认信号
  - 每一个Block都会有一个检验码(CheckSum)，在读文件的时候保证数据完整性
- Step6: HDFS Client收到确认信号后，向NameNode发送确认信息，此时文件成功写入HDFS
  - 如果某个DataNode写入失败，数据会继续写入到其他DataNode。最终NameNode会重新选择一个DataNode进行数据复制

![img](/images/Hadoop之HDFS写入.png)

### 读取操作

- Step1: 客户端向NameNode发送读取请求
- Step2: NameNode向HDFS Client返回文件的所有Block和Block所在的DataNode(包括复制DataNode)信息
- Step3: HDFS Client从这些DataNode中读取数据，如果读取失败，则从复制节点中读取

![img](/images/Hadoop之HDFS读取.png)

### 可靠性

- DataNode可以失效: DataNode和NameNode之间通过定时心跳进行通信。如果一段时间内NameNode没有收到DataNode心跳信息，则认为其失效，此时NameNode将该DataNode中的Block数据复制到其他的DataNode中
- Block可以失效: 当写入或者硬盘出现问题导致Block数据失效时，可以通过从其他的DataNode进行读取并复制
- NameNode不可以失效: 当NameNode失效时，整个HDFS失效

## YARN

是一种新的MapReduce框架

### 原有的MapReduce结构

- JobTracker: 负责资源管理，跟踪资源消耗情况和资源可用性。作业生命周期管理，包括作业任务调度，任务进度跟踪，任务容错机制等
- TaskTracker: 加载和关闭具体的任务，定时报告任务的状态

- 存在问题: 单点性能瓶颈和资源利用率底下
  - JobTracker是MapReduce任务的集中处理节点，存在单点故障问题
  - JobTracker负担过重，造成过多资源消耗。当MapReduce的Job非常多时，会造成很大的内存开销，成为系统性能瓶颈
  - TaskTracker以MapReduce Task的数目作为资源表示太过简单，没有考虑到CPU/内存等占用情况，使得TaskTracker中容易出现OOM
  - TaskTracker将资源强制划分为Map Task Slot和Reduce Task Slot，当系统中只有一种Task时会造成资源的浪费

![img](/images/Hadoop之旧MapReduce结构.jpg)

### YARN结构

将JobTracker资源管理和任务调度与监控的职责拆分为两个独立的进程: 全局资源管理和单个Job的管理

- ResourceManager: 全局资源管理与任务调度
- NodeManager: 单个节点的资源管理与监控
- ApplicationMaster: 单个Job的资源管理和任务监控
- Container: 资源申请的单位和任务运行的容器

![img](/images/Hadoop之YARN结构.png)

![img](/images/Hadoop之YARN物理结构.png)

### 新旧结构对比

在YARN结构下形成了一个通用的资源管理平台和一个通用的应用计算平台，避免了旧结构下的单点问题和资源利用率低下的问题，同时也让其上运行的应用不再局限于MapReduce形式

![img](/images/Hadoop之结构对比.png)

### YARN运行流程

- Step1: 提交Job
  - 从ResourceManager中获取一个Application ID
  - 检查Job输出配置，计算Job输入Block
  - 将Job资源(Jar包、配置文件、Block信息等)上传到HDFS

- Step2: 初始化Job
  - ResourceManager将Job递交给Scheduler
  - Scheduler根据调度算法将为Job分配一个Container，同时ResourceManager创建一个ApplicationMaster并交给NodeManager管理
  - ApplicationMaster获取Job的Block信息，为每一个Block创建一个MapTask和ReduceTask，同时创建一系列监控进程来跟踪Job的进度和状态

- Step3: 指派Task
  - ApplicationMaster向ResourceManager申请分配Container资源，ResourceManager一般根据数据本地化原理来进行资源分配

- Step4: 执行Task
  - ApplicationMaster根据ResourceManager的资源分配情况在对应的NodeManager中启动Container来运行Task
  - 在Container中，从HDFS上获取Task运行所需要的资源(Jar包，配置文件等)，然后运行该Task

- Step5: 上报Task状态和进度
  - 在Container中，定时将Task的进度和状态上报给ApplicationMaster
  - Client定时向ApplicationMaster获取整个Job的进度和状态

- Step: 完成Job
  - Client定时检查整个Job是否完成
  - 在作业完成后进行资源的清理操作

![img](/images/Hadoop之YARN运行流程1.png)

![img](/images/Hadoop之YARN运行流程2.png)

### ResourceManager

全局资源管理与任务调度，将整个集群看做计算资源池，只关注资源分配，不负责应用的运行和容错

- 资源管理
  - 在旧结构中，每一个Node分为一个个的MapSlot和ReduceSlot；在YARN中是一个个Container，每个Container可以根据需要运行ApplicationMaster、Map、Reduce或者任意其他的程序
  - 在旧结构中，计算资源是静态分配的；在YARN中，Container是动态分配的，资源利用率更高
  - Container是计算资源申请和分配的单位，申请格式 `resource-name, priority, resource-requirement, number-of-containers`
    - resource-name: 主机名、机架名或*（代表任意机器）
    - resource-requirement: 目前只支持CPU和内存
  - 当Client提交Job到ResourceManager时，首先ResourceManager在某个NodeManager上分配一个Container来运行ApplicationMaster，ApplicationMaster再根据Job实际需要向ResourceManager申请运行Task的Container

- 任务调度
  - Scheduler根据集群资源的使用情况和资源申请的需求来合理分配计算资源
  - Scheduler在分配计算资源时，会通过考虑数据本地化减少数据移动，提高Job的运行效率
  - Scheduler支持在特定的机器上申请特定的资源

- 具体模块
  - Client Service: Application提交、终止、输出信息（应用、队列、集群等的状态信息）
  - Administration Service: 队列、节点、Client权限管理
  - ApplicationMasterService: 注册、终止ApplicationMaster, 获取ApplicationMaster的资源申请或取消的请求，并将其异步地传给Scheduler, 单线程处理
  - ApplicationMaster Liveliness Monitor: 接收ApplicationMaster的心跳消息，如果某个ApplicationMaster在一定时间内没有发送心跳，则被任务失效，其资源将会被回收，然后ResourceManager会重新分配一个ApplicationMaster运行该Application（默认尝试2次）
  - Resource Tracker Service: 注册节点, 接收各注册节点的心跳消息
  - NodeManagers Liveliness Monitor: 监控每个节点的心跳消息，如果长时间没有收到心跳消息，则认为该节点无效, 同时所有在该节点上的Container都标记成无效，也不会调度任务到该节点运行
  - ApplicationManager: 管理Application，记录和管理已完成Application
  - ApplicationMaster Launcher: 一个Application提交后，负责与NodeManager交互，分配Container并加载ApplicationMaster，也负责终止或销毁
  - YarnScheduler: 资源调度分配， 有FIFO(with Priority)，Fair，Capacity方式
  - ContainerAllocationExpirer: 管理已分配但没有启用的Container，超过一定时间则将其回收

![img](/images/Hadoop之YARN-ResourceManager.png)

### NodeManager

- Node中的Container管理和监控
  - 启动时向ResourceManager注册并定时发送心跳
  - 监视Container的运行和资源使用情况，维护Container的生命周期
  - 启动和停止Container，管理Task运行时的依赖(从HDFS中将Jar包、配置文件等拷贝到本地)

- 具体模块
  - NodeStatusUpdater: 启动时向ResourceManager注册，报告该节点的可用资源情况，通信的端口和后续状态的维护
  - ContainerManager: 接收RPC请求(启动停止Container)，资源本地化(下载应用需要的资源到本地，根据需要共享这些资源）
    - PUBLIC: /filecache
    - PRIVATE: /usercache//filecache
    - APPLICATION: /usercache//appcache//（在程序完成后会被删除）
  - ContainersLauncher: 加载或终止Container
  - ContainerMonitor: 监控Container的运行和资源使用情况
  - ContainerExecutor: 和底层操作系统交互，加载要运行的程序

![img](/image/Hadoop之YARN-NodeManager.png)

### ApplicationMaster

- 单个Job的资源管理和任务监控
  - 计算Application的计算资源需求，可以静态或动态计算
    - 静态: Client在提交Job时指定
    - 动态: ApplicationMaster根据Job输入Block数量来决定MapTask和ReduceTask的数量，一个Task占用一个Container
  - 根据数据本地化向ResourceManager申请对应位置的Node资源
  - 与NodeManager交互实现Task的运行和Task状态与进度的监控
  - 向ResourceManager包括Job的状态和进度
  - 负责Job运行时的容错

### Container

- 集群中基本的资源申请和分配单位
- 可以运行任意程序，不局限为Jar包
- 一个Node中可以包含多个Container
- ApplicationMaster根据Job的实际需要，动态申请和释放Container资源

## MapReduce

一种分布式的计算方式

基本流程: ![img](/images/Hadoop之MapReduce基本流程.png)

详细流程: ![img](/images/Hadoop之MapReduce详细流程.png)

跨节点流程: ![img](/images/Hadoop之MapReduce跨节点流程.png)

具体过程:

- 读数据: 通过InputFormat决定读取的数据类型，然后拆分为一个个的InputSplit，每一个InputSplit被RecordReader读取并传递给Map进行处理
  - InputFormat: 决定读取数据的格式，可以为文件或数据库
  - InputSplit: 代表数据的逻辑分片，内部包含数据的Location信息，方便利用数据本地化来申请Container资源 **通常一个InputSplit就是一个Block**
  - RecordReader: 读取InputSplit，并交给一个Map来处理

- Map: 读取InputSplit中的每一个KV对并进行处理

- Shuffle: 对Map的结果进行压缩、按Key进行分区、排序等操作，然后传递给Reduce

- Reduce: 对Shuffle的结果进行处理，最终将Reduce的结果写入HDFS中